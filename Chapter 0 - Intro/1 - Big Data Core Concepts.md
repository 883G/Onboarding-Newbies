# Introduction to Core Data Concepts :baby:

## **Goals**
- Understand the data landscape and its core concepts.
- Explore the importance of data operations and the role of data in guiding business decisions.
- Gain a comprehensive overview of the data lifecycle and its implications for the organization.

## **Overview**
Today’s session lays the foundation for your journey into data operations. You will examine the core concepts that underpin the modern data landscape and gain insight into data management, processing, and analysis. The emphasis is on appreciating how data drives business decisions and shapes the strategic direction of an organization.

> ⚠️ **Note:**  
> This is your first day in the world of data operations. Don’t worry about diving deeply into every topic; focus on understanding the big picture and the major concepts.

## 1. Understanding the Data Landscape

Research the following topics and look for real‑world examples. Discuss your findings with your mentor to deepen your understanding and clarify any questions. Keep a high‑level perspective, and consider the drawbacks of each concept as well as alternatives.

1. **The five V’s of Big Data** :


the five v's of big data - חמשת העקרונות הם 5 כלים שעוזרים לנו לנהל ולנתח כמות ענקית של נתונים שמתעדכנת בקצב מהיר.
	1.1 volume - חברות היום אוגרות כמות ענקית של נתונים, כמות שגדולה הרבה יותר מדיי כדי לשמור אותה על שרת לוקלי, במקום זה משתמשים בdata lakes, צורה לשימור מידע בכמות גדולה מאוד בצורה מבוזרת ובעלות נמוכה בגלל שהנתונים נשמרים בצורה ה"מקורית" שלהם והנפח נמוך יותר.
	1.2 velocity - נתונים גדלים ומתעדכנים בקצב מהר מאוד, הם משתנים וגם מתיישנים. המאגר מידע צריך למצוא דרך וכלים שיוכלו לעמוד בקצב של השינויים ולהתעדכן בהתאם.
	1.3 variety - צריך לדעת לפעול עם סוגים שונים של נתונים, נתונים רלציונים, לא רלציונים וגם מידע שמסודר בטבלאות וכאלה שלא. יש צורך לדעת איך לנתח אותם לסנן ולמצוא אותם בזמן אמת.
	1.4 value - לנתונים שאנחנו שומרים חייב להיות ערך, אנחנו צריכים לתת להם משמעות בעזרת מה שאנחנו יוצרים מהם, בשביל זה צריך שקיפות,
	1.5 veracity - חייב לאמת את הנתונים שלנו כדי שהעבודה שלנו עם הדאטה בייס תהיה אמינה. עבודה עם הנתונים טובה ככל שתהיה לא תנפיק תוצאות טובות אם יש שגיאה בנתונים, באג בתוכנה שגורם לבעיה בנתונים, נתונים מיותרים וכו.. וגם אם למישהו לא מוסמך יש אופציה לערוך אותם.
איך נראה הדאטה לייק?


2. **Structured, unstructured, and semi‑structured data**

structured, unstructured and semi-structured data -
	2.1 structured data - מידע המסודר בסכמות, בדרך כלל טבלאות, אפשר לאחסן אותו בדאטה בייס רלציוני ולתשאל אותו בצורה פשוטה ומהירה. בדר"כ משתמשים בפוסטגרס, MYSQL וכו…
פוסטגרס הוא טיפה יותר גמיש עם הסוגי מידע ומה שאפשר לעשות איתם, מספק יותר אבטחה למידע.
	2.2 semi structured data - מידע שמסודר בצורה מסוימת אבל לא בהכרח בסכמה מסודרת, נגיד קבצי JSON או YAML, אימיילים. בשביל זה משתמשים בדאטה בייס NOSQL  כמו מונגו וכלים כמו hadoop שהוא כלי מבוסס קוד פתוח שמאפשר שמירה של כמויות גדולות מאוד של נתונים על שרתים נורמטיביים לפי 2 עקרונות בסיסיים: 
אחסון מבוזר (HSDF)- מפצלים קבצים גדולים לבלוקים ומשכפלים אותם בין שרתים שונים.
עיבוד מקבילי (MAPREDUCE) - מודל תכנות לעיבוד נתונים במקביל, מפצלים את המשימה לתתי משימות לכמה שרתים ומאחדים בסוף את התוצאה מכולם.
 וגם elastic - מנוע חיפוש מתקדם מבוסס קוד פתוח בJAVA ומאפשר חיפוש של נתונים מכל סוג שהו, בין אם זה סכמתי או לא או אם זה נתון גיאוגרפי ויזואלי מספרי וכו…
	2.3 unstructured data - מידע שלא מסודר בצורה מסויימת, יכול להיות כמות גדולה של מידע שקשה יותר לעכל, יכול להיות תמונה, סרטון או טקסט. גם בשבילו נשתמש בהדופ ועוד כלים של למידת מחשב כמו spark שהוא מודל לניתוח כמות גדולה של מידע לא מבוזר.


3. **ETL vs. ELT**


	 ETL extract, transform, load - שיטה לעיבוד מידע בה צריך לקלוט מידע מהרבה מקורות שונים ולמיין ולסנן אותה בשביל ניתוח. הצעד הראשון הוא לאסוף מידע ממקורות שונים (מאגרי מידע, קבצים וכו..), הצעד השני הוא לשנות את הנתון לפורמט שצריך בשביל המערכת שלנו ו"מנקים" אותה, מוציאים מידע לא הכרחי ממנה ומנקים כפילויות. הצעד השלישי הוא לטעון את המידע לדאטה בייס שלנו.
ELT - extract, load, transform - שלב 1 זהה לETL, אבל טוענים את הנתונים לדאטה בייס ורק אז עושים להם טרנספורמציה לפורמט שצריך, למה? כי אז אפשר לעשות את הטרנספורמציה רק מתי שצריך את הנתונים האלה ולא לעשות פעולות על נתונים שלא בהכרח ניגע בהם (הכל עולה לנו זמן ומקום).
נעדיף להשתמש בETL כשצריך להצפין מידע או כשהטרנספורמציה שצריך לעשות עליו מסובכת מאוד ולא רוצים לעשות אותה ברגע שצריך את הנתון כי היא מורכבת.
נעדיף להשתמש בELT כשיש כמויות ענק של מידע והרבה פעמים צריך את המידע המקורי ללא טרנספורמציות.


4. **NoSQL vs. SQL databases**


	4.1 SQL database - מאגר נתונים רלציוני שמבוסס SQL, מסודר בסכימות שאפשר לתשאל ולבצע פעולות עליהן בעזרת הSQL. המידע נשמר בטבלאות וקשרים בין הטבלאות. בסוג כזה של דאטה בייס כל נתון חייב להיות מסוג מסויים ואי אפשר לעבד את כל סוגי הנתונים.
דוגמאות לSQL דאטה בייס: MYSQL , POSTGRESQL, ORACLE.
	4.2 NoSQL database - מאגר נתונים לא רלציוני שיכול לשמור סוגים שונים של נתונים (לא בהכרח סכמות) וכדי לנתח אותו או לבצע עליו פעולות אי אפשר להשתמש בשאילתות SQL רגילות.
סוגי NoSQL דאטה בייס:
דאטה בייס מסמכים - שומר מידע במסמכים בדומה לJSON.
Key value - שומר כל מידע עם מפתח וערך.
דאטה בייס גרף - כל נתון הוא קודקוד וקשרים בין נתונים הם קשתות.


column based - דאטה בייס שמאורגן בעמודות, HBASE.



6. **OLAP vs. OLTP**

גם OLAP וגם OLTP שתיהן גישות לשאילתות מורכבות.
OLAP מטרתו בעיקר לאסוף נתונים מצטברים בכמות גדולה לניתוח בעוד בOLTP מטרתו בעיקר לנהל ולבצע עסקאות והעברות בכמות גדולה ובזמן קצר.
בOLAP יעדיפו לבצע קריאת נתונים מאשר כתיבת נתונים ואפשר לבצע שאילתות בצורה מהירה ופשוטה. אבל הזמן עיבוד של OLAP יכול לקחת משמעותית יותר זמן.
OLTP מתעדף כתיבה על קריאה והוא מהיר מאוד (זמן עיבוד של מילישניות).
רוב הפעמים יעשו שילוב של השתיים כדי שיהיה גם ניתוח וגם העברת עסקאות ברמה גבוהה.


6. **Batch processing vs. stream processing**

שתי דרכים לעיבוד נתונים
	6.1 Bach processing - עיבוד של כמות גדולה של מידע בבת אחת באופן חוזר, אין צורך לשלוח תוצאות ישר וזה חוסך עלות מהמחשב. בסוף העיבוד הוא מחזיר דוח מסודר עם כל התוצאות והניתוחים שצריך לדעת. HIVE, SPARK.
Hive - מציג ניתוח נתונים בצורת שאילתות SQL, בנוי על בסיס האדופ.
SPARK - מעבד נתונים שיכול לרוץ על האדופ או עצמאית, מהיר מאוד.
	6.2 stream processing - עיבוד בצורה תכופה וחוזרת של נתונים, תגובה ישירה לעדכונים, מהיר אבל קשה להשמה. נדע ישר כשיש שגיאה או תקלה. KAFKA, STORM,FINK.
KAFKA - כלי ששומר את העדכונים ואת ההיסטוריה שלהם.


7. **Data warehouse vs. data lake**


data warehouse - זה סוג של דאטה בייס ונועד לאגור סיכום של הנתונים על מנת שיוכלו לבצע ניתוח של הנתונים. הוא שומר עדכון של הנתונים כל כמה זמן ותמיד את ההיסטוריה שלהם, אבל לא תמיד יהיה מעודכן בדיוק באותו רגע. זה עדיף לניתוח נתונים.
data lake - זה לא דאטה בייס אלא מאגר שאליו אפשר "לזרוק" את כל סוגי הנתונים שיש, רלציונים מובנים או אף אחד מהם, יותר נועד לשימוש של למידת מכונת וAI מאשר ניתוח נתונים כמו שהוא כי זה קשה יותר.


8. **Distributed file systems**

 Distributed file systems - מערכת קבצים שנפרסת על כמה שרתים או מיקומים ועדיין הקבצים נגישים מכל מכשיר בכל מקום (כל עוד יש הרשאה). ככה כמות הקבצים שאפשר לשמור היא עצומה (כי תמיד אפשר להוסיף בלוק עם עוד קבצים על שרת נוסף) ואפילו אם אחד מהשרתים נפל לא נאבד כל המידע שלנו כי פיזרנו את הנתונים בין שרתים ובין מקומות. דוגמאות לDFS:
HDFS, NFS, SMB. לHDFS יש קיבולת גבוהה יותר לקבצים ויש בה כפילויות של קבצים כדי להחזיק מנגנון חירום במקרה של כשל באחד/ כמה שרתים.



9. **Data governance**

 data governance - עיקרון פיקוח על נתונים, בדיקה שהם מדויקים ומטופלים כראוי בזמן שהם עוברים מניפולציות, מקבלים קלט, נמחקים ועוברים עוד שינויים.
ארגון הפיקוח מבטיח שפוליסות יוגדרו, הטכנולוגיות מנוהלות כראוי, והנתונים מוגנים עוד לפני שהם מוזנים במערכת וגם במהלך השימוש בהם.
ישנם ארגונים שבהם יש אדם שתפקידו לפקח על שלמות הנתונים.


10. **Data visualization**

Data visualization - שימוש בדימוי חזותי של נתונים כדי להסביר אותם בגרפים ואת היחסים ביניהם.
יש 5 סוגים של תיאור יחסי נתונים:
היררכיה (מדד בנקודת זמן), סידוריות (רצף בנקודת זמן), דומיננטיות, מגמה (שינוי בציר), וצפיפות.
כלים - tableau, looker וכו..


11. **Data analytics**

Data analytics - תהליך לניקוי וטרנספורמציה של הנתונים כדי לגלות מהם מידע שיעזור לבסס החלטות עליו.
בתוך ניתוח הנתונים משתמשים בdata mining שזה תהליך למציאת תבניות בנתונים בעזרת למידת מכונה, סטטיסטיקה ועוד כדי לחלוץ נתונים מהנתונים שיש.
ניתוח נתונים מחולק לEDA ולCDA.
EDA זה שיטה לסכם את המאפיינים המרכזיים של נתונים בעזרת דימוי חזותי וגרפים של נתונים.
CDA זה שיטה בה בעזרת הנתונים מחליטים אם אפשר לדחות תיאוריה מסויימת או לא.
בשביל ניתוח נתונים צריך:
איסוף נתונים, עיבוד נתונים, ניקוי נתונים ואז ניתוח בעזרת  EDA , פונקציות מתמטיות ומודלים


12. **Data ownership**

למי יש הרשאה לגשת או לשנות את הנתונים ועבור איזה מטרות, זה כולל ניהול הרשאות ופרטיות.
חשוב כדי למנוע מניפולצית מידע שלא אמורה לקרות למטרות זדוניות, כדי להגן על הנתונים ולשמור עליהם בצורה מסודרת.


13. **Data quality**


מדידה של כמה הנתונים מתאימים לקריטריונים הנדרשים. דאטה יוחשב באיכות גבוה אם היא מתאימה לקבלת החלטות ותכנון כמו שצריך וגם נמדדת לפי שלמות הנתונים.
מדדים:
שלמות הנתונים, נגישות\זמינות (כמה מהר אפשר לגשת לנתונים), השוואתיות (אם אפשר להשוות בין נתונים), נכונות ואמינות, רלוונטיות, כמה הנתון מיוחד.
יש סטנדרט בינלאומי לאיכות הדאטה בשם ISO 8000.


14. **CDC (Change Data Capture)**

CDC זה זיהוי ותיעוד אוטומטי כדי לעקוב אחריי שינויים בנתונים והעברתם למקום אחר בעזרת סנכרון מערכות.
זה משומש ב data warehouse בדרך כלל בשביל לשמור שינויים ואת המצב הנוכחי של הדאטה.
טבלאות במשתנות חייבות להיות מתועדות ולפעמים יש עמודה של מתי הנתונים שונו או הגרסה האחרונה. 


15. **Data catalog**


שמירה של כל הנתונים השמורים בארגון מסויים, זה לא דאטה בייס כי הוא שומר מידע על הנתונים (מטה דאטה) , זה יותר מערכת השומרת את כל הנתונים שיש ונותנת אופציה לחפש אותם ולראות באיזה נתונים חברה משתמשת ובשביל איזה מטרות.


16. **Data lifecycle management**

שיטה לניהול הנתונים שהחברה מגדירה מהרגע שהם הגיעו ועד לרגע שהם הושמדו. בתהליך זה צריך לתעדף הגנה על הנתונים ומניעת מחיקה מלא רצויה שלהם.
שלבים:
יצירת הנתונים- תמיד צריך לבדוק את איכות הנתונים קודם.
אחסון הנתונים- המידע נשמר בשיטת כזו או אחרת ומיד צריך לעשות סריקת חולשות כוללת לכל הנתונים.
שיתוף ושימוש בנתונים- בDLM מגדירים מי יכול להשתמש בדאטה.
ארכיון נתונים- העברת נתונים ישנים יותר לארכיון , החברה צריכה להגדיר בDLM שלה אחריי כמה זמן הנתונים עוברים ולכמה זמן הם עוברים.
מחיקת דאטה- מחיקת של דאטה שכבר אין שימוש בו רק למטרת פינוי מקום לנתונים אחרים.

17. **Data lineage**


תהליך של שחזור כל מחזור החיים של נתון, מציאת הרקורד המקורי והשינויים שנעשו מאז עד לתוצאה הסופית.


18. **Store‑first approach**

עקרון שמתעדף לשמור קודם נתונים באופן לוקלי על המכשיר של הלקוח או בשכבה המיועדת לכך ורק אז לעבד אותו כדי שאותו נתון יהיה זמין גם כשאנחנו עושים עליו טרנספורמציות לפני עיבוד.
זה עוזר בגישה מהירה לקובץ ומהירות אבל אם הקובץ גדול זה מקשה מאוד, לוקח מקום ומסבך את התהליך.


19. **Data serialization**

תהליך של הפיכת נתון לפורמט שאפשר לשמור.
בדרך כלל משתמשים בJSON, XML,YAML.


20. **Data compression**

שיטה לקידוד ומניפולציה על נתונים כדי להקטין את הגודל שלהם.
ZIP,RAR,7Z,MP3
אפשר לעשות זאת ע"י מחיקת נתונים מיותרים ואפשר לעשות זאת על ידי דחיסה של הנתונים בצורה שלא מאבדת שום נתון.


21. **Scale‑out vs. scale‑up**

21.1 scale-out - מוסיפה עוד שרתים כדי לפזר להם את העבודה ובכך להגביר את הכוח ולהוריד מהעומס על השאר (כל השרתים מהירים יותר) אבל מסובך יותר לבצע את זה ויסבך אם הנתונים יהיו מאוד מפוזרים, יכול להיות תלוי גם בזמן שהנתונים עוברים בין השרתים.
	21.2 scaling-up - לחזק שרת קיים , להוסיף לו משאבים (CPU,RAM) וככה להעצים את הפעילות שלו, עדיין מוגבלים מבחינת המשאבים של אותו שרת.


22. **High availability**


עקרון שאומר שצריך להיות נגישות באופן רציף למשאבים. גם במקרה של תקלות צריך למזער את מספר ההשבתות למינימום כמעט מוחלט תוך כדי שימוש בגיבויים,עולה הרבה משאבים וכוח אבל הכרחי.


23. **Master‑slave vs. masterless architectures**

23.1 master-slave - כל פעולות הכתיבה נעשות ע"י דאטה בייס ראשי אחד ואת הנתונים אחריי השינויים מעדכנים לדאטה בייסים האחרים אחריי שהפעולה נעשתה.
לדאטה בייס הראשי עושים את כל ההכנסות מחיקות עדכונים וכו והשאר רק מקבלים העתק של הדאטה בייס שלו. פעולות השאילתות נעשות על הדאטה בייסים הלא ראשי.
במקרה שהדאטה בייס הראשי נופל תמיד יש את אחד מהמשניים שיחליף אותו והמערכת תמשיך לתפעל כרגיל, וההקבלה והפרדה של הכתיבה וקריאה מייעלת ומזרזת את התהליך. לעומת זאת יכול להיות לאגינג ושהדאטה בייסים המשניים לא יעמדו בקצב עם השאר או שיהיו יותר מדיי פעולות כתיבה והדאטה בייס הראשי לא יעמוד בזה.

23.2 masterless - כל הדאטה בייסים יכולים לבצע כתיבה וקריאה, מידע משוכפל כמה פעמים בין טבלאות כדי ליצור גיבויים והתאמה בנתונים. זה מוריד עומס מטבלה אחת ויותר יציב אם אחת הטבלאות קורסת אבל גם יותר קשה לביצוע ויש אופציה שנעשות שני עדכונים מקבילים שנוגדים אחד את השני וצריך לטפל בכך.


24. **Apache data stack**


APACHE היא חברה שיוצרת כלים מבוססים קוד פתוח שמאפשרים לנהל מידע בכמויות ענק מהעיבוד שלהם לניתוח שלהם.
כלים לעיבוד:
kafka, NiFi ועוד..
כלים לאחסון ופיקוח:
HADOOP,HIVE, ICEBERG
ביצוע פעולות על הנתונים:
SPARK, AIRFLOW
ניתוח נתונים:
SUPERSET, DRUID.

These topics are meant to guide your research. Don’t hesitate to look up other relevant concepts.
</br>
> Note✅: Reinforce your understanding by relating the concepts to real‑world scenarios.

## Wrapping Up

### Reflection
Take a few minutes to reflect on what you have learned:
- jot down key takeaways
- note any questions or uncertainties

### Mentor Discussion
Talk through the following with your mentor:
- clarify concepts that remain unclear
- share your findings and insights
- discuss real‑world use cases and implications for your work

## Q&A Session :raising_hand:
Participate in an open Q&A session with your mentor to address any questions about specific tools, technologies, or practices.

## Action Items
- Identify areas you want to explore more deeply.
- Ask for recommended resources for further learning.
