
# ShowCase: In-Depth Exploration of Hadoop Ecosystem Component(s) :book:

**Est. Time:** 8 hours

:hand: __**ASK YOUR MENTOR**__ - *This assignment may be different for you depending on your onboarding process.*

## Goals:
- Gain a foundational understanding of the Hadoop ecosystem.
- Explore key components and their roles in big data processing.
- Make connections between the Hadoop ecosystem and real-world use cases.
- Develop a high-level understanding of the Hadoop ecosystem and its role in big data processing.
- Familirize yourself with the Hadoop ecosystem and its role in big data processing.
- Start to work with time estimation and planning.
- Learn how to work with research and documentation.
- Learn how to work with presentations.

:warning: **Note:**
- This is a self-study day, it is important to be able to work independently and manage your time.
- A lot of newbies have a problem with self-study and time management, so take a breath and take your time to plan your day carefully.
- Be aware of the importance of time management. Plan your time 
and tasks, and try to stick to your plan. If you are not able to finish the tasks in the time you have planned, you should discuss it with your mentor, maybe you learn more then you should.
- Maybe This is the first time you are working with research and documentation, so take your time to read the instructions carefully and ask your mentor if you have any questions.

### Introduction

For this assignment, you are tasked with exploring one or more components of the Hadoop ecosystem in-depth and preparing a research document and presentation to share with the team the next day. This assignment aims to deepen your understanding of specific Hadoop ecosystem components and their significance in big data processing and analytics.

### Instructions

1. **Choose a Hadoop Ecosystem Component(s):**
   - Select one or more components from the list provided below or propose your own if you have a specific interest.
   - The goal is to gain an in-depth understanding of the chosen component(s).

2. **Research and Document:**
   - Spend time researching the selected Hadoop ecosystem component(s).
   - Create a research document that includes the following sections:
     - Introduction: An overview of the chosen component(s).
     - Architecture and Components: Explain the architecture and main components.
     - Features and Capabilities: List and detail core features and capabilities.
     - Use Cases: Explore real-world use cases where the component(s) are applied.
     - Implementation and Configuration: Provide insights into setup and configuration.
     - Challenges and Considerations: Identify challenges and possible mitigations.
     - Future Developments: Discuss ongoing developments or future enhancements.
     - Conclusion: Summarize key findings and the component(s)' significance.
     - Use our [Research Template](research_doc_template.md) as a starting point for your document and presentation.

3. **Prepare a Presentation:**
   - Create a presentation based on your research document.
   - Prepare slides covering the key points from each section of your research.
   - Be ready to present your findings to the team.

4. **Present to the Team:**
   - The next day, present your research findings to your team.
   - Encourage questions and discussions to deepen everyone's understanding.
   
### Examples of Hadoop Ecosystem Components

1. **NameNode Architecture:** Explore the structure and functionality of the NameNode in HDFS.
2. **FairScheduler:** Understand the workings and advantages of the FairScheduler in YARN for resource allocation.
3. **CapacityScheduler:** Delve into the CapacityScheduler, focusing on its resource allocation strategies and queue management.
4. **Catalyst Optimizer:** Investigate the role and benefits of the Catalyst Optimizer in Apache Spark's execution engine.
5. **FsImage and EditLog:** Examine how HDFS uses FsImage and EditLog for namespace and metadata storage.
6. **Impala Query Execution:** Delve into the architecture and performance characteristics of Apache Impala, a massively parallel processing (MPP) SQL query engine for Hadoop.
7. **Hive Metastore:** Understand the structure and purpose of the Hive Metastore in managing metadata for Hive tables.
8. **HBase Compaction:** Discuss the process and significance of compaction in HBase for optimizing storage and performance.
9. **Spark Streaming:** Explore the architecture and use cases of real-time data processing with Apache Spark Streaming.
10. **Kafka Producer and Consumer APIs:** Delve into the mechanisms of producing and consuming messages in Apache Kafka using its Producer and Consumer APIs.
11. **Invalidate Metadata:** Understand the process of invalidating metadata in Impala for ensuring data consistency and accuracy.
12. **Your Choice:** Fill free to propose your own topic if you have a specific interest.

Remember to thoroughly research, document, and present your findings to ensure a meaningful exploration of your chosen Hadoop ecosystem component(s).
